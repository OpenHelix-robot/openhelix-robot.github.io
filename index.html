<!DOCTYPE html>
<html lang="en">
<head>
   <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>OpenHelix</title>
    <meta name="description" content="OpenHelix Project Page">
    <!-- Bootstrap 3 -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <!-- FontAwesome for icons -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <style>
        body {
            padding-top: 50px;
        }
        .text-justify {
            text-align: justify;
        }
        .img-responsive {
            max-width: 100%;
            height: auto;
        }
        .link-block {
            margin: 5px;
            display: inline-block;
        }
        .btn-custom {
            border-radius: 25px;
            background-color: #333;
            color: white;
        }
        .btn-custom:hover {
            background-color: #555;
            color: white;
        }
    </style>
</head>
<body>
<div class="container">
    <div class="row text-center">
        <h1><strong>OpenHelix</strong>: A Short Survey, Empirical Analysis, and Open-Source Dual-System VLA Model for Robotic Manipulation</h1>
        <p>Can Cui, Pengxiang Ding*, Wenxuan Song, Shuanghao Bai, Xinyang Tong, Zirui Ge, Runze Suo<br>
         Wanqi Zhou, Yang Liu, Bofang Jia, Han Zhao, Siteng Huang, Donglin Wang†</p>
        <p>Westlake University, Zhejiang University, Xi'an Jiaotong University, HKUST(GZ) </p>
        <p>*Project lead | †Corresponding author</p>
        <p><strong>Under Review</strong></p>
    </div>
    <div class="row text-center" style="margin-top: 20px;">
      <!-- PDF Link -->
      <span class="link-block">
        <a href="https://github.com/OpenHelix-robot/awesome-dual-system-vla/" class="btn btn-custom" target="_blank">
          <i class="fas fa-file-pdf"></i> Awesome Survey
        </a>
      </span>

      <!-- arXiv Link -->
      <span class="link-block">
        <a href="https://openhelix-robot.github.io/" class="btn btn-custom" target="_blank">
          <i class="fas fa-book"></i> ArXiv
        </a>
      </span>

      <!-- Code Link -->
      <span class="link-block">
        <a href="https://github.com/OpenHelix-robot/OpenHelix" class="btn btn-custom" target="_blank">
          <i class="fab fa-github"></i> Code
        </a>
      </span>

      <!-- Huggingface Link -->
      <span class="link-block">
        <a href="https://openhelix-robot.github.io/" class="btn btn-custom" target="_blank">
          <i class="fas fa-smile-beam"></i> Huggingface(coming soon)
        </a>
      </span>
    </div>
<!--     <div class="row text-center">
        <ul class="list-inline">
            <li><a href="#" target="_blank"><strong>Paper</strong></a></li>
	    <li><a href="https://github.com/OpenHelix-robot/OpenHelix" target="_blank"><strong>code</strong></a></li>
	    <li><a href="#" target="_blank"><strong>checkpoints</strong></a></li>
        </ul>
    </div> -->

    <div class="row">
        <div class="col-md-8 col-md-offset-2">
            <h3>Abstract</h3>
            <p class="text-justify">
                Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from.
            </p>


            <h3>Key Design of Dual-system VLA </h3>
            <p class="text-justify">
                Dual-system VLA (Vision-Language-Action) architectures have become a hot topic in embodied intelligence research, but there is a lack of sufficient open-source work for further performance analysis and optimization. To address this problem, this paper will summarize and compare the structural designs of existing dual-system architectures, and conduct systematic empirical evaluations on the core design elements of existing dual-system architectures. Ultimately, it will provide a low-cost open-source model for further exploration. Of course, this project will continue to update with more experimental conclusions and open-source models with improved performance for everyone to choose from.
            </p>
            <!-- <p style="text-align: center;"><img src="./assets/comp_table.png" class="img-responsive"></p> -->
            <p style="text-align: center;"><img src="./assets/overview.png" class="img-responsive"></p>

            <h3>Empirical Study of Key Design</h3>
            <!-- <h4>Training strategy of dual system</h4> -->
          
            <!-- <p style="text-align: center;"><img src="./assets/Key Design_1.png" class="img-responsive"></p>
            <p style="text-align: center;"><img src="./assets/Key Design_2.png" class="img-responsive"></p> -->

            <!-- <h4>Testing strategy of dual system </h4> -->
            <p class="text-justify">
                The results show that the current MLLM is insensitive to environmental changes, which is unexpected. Analysis of action token embeddings reveals that they mainly reflect instruction semantics, with little adaptation to visual input. For example, “right” consistently has a higher similarity than “left,” regardless of actual motion. This suggests the MLLM does not effectively leverage visual reasoning and instead passes static instruction semantics to the policy. Thus, current dual-system designs are flawed, as the MLLM’s visual guidance is not meaningfully transferred to the action policy—raising the question of whether a single LLM could achieve similar performance.
            </p>
            <p style="text-align: center;"><img src="./assets/finding.jpg" class="img-responsive"></p>

            <h3>Open-Source Framework</h3>
            <p class="text-justify">
                OpenHelix bridges the high-level MLLM and the low-level policy using a learned <ACT> token and linear projection. Prompt tuning allows efficient training without altering MLLM parameters. An auxiliary task ensures the MLLM performs multimodal reasoning by predicting actions directly from its latent embeddings. The policy model adopts a diffusion-based learning mechanism conditioned on visual, proprioceptive, and goal features.
            </p>
            <p style="text-align: center;"><img src="./assets/arch.jpg" class="img-responsive"></p>

            <h3>Final Result</h3>
            <p class="text-justify">
                OpenHelix bridges the high-level MLLM and the low-level policy using a learned <ACT> token and linear projection. Prompt tuning allows efficient training without altering MLLM parameters. An auxiliary task ensures the MLLM performs multimodal reasoning by predicting actions directly from its latent embeddings. The policy model adopts a diffusion-based learning mechanism conditioned on visual, proprioceptive, and goal features.
            </p>
            <p style="text-align: center;"><img src="./assets/FInal_Result.png" class="img-responsive"></p>


            <!-- <h3>Citation</h3> -->
            <!-- <pre><code>@article{Cui2024OPENHELIX,
  title={OpenHelix: A Simple yet Effective Dual System for Robot Learning},
  author={Can Cui and Pengxiang Ding and Wenxuan Song and Hangyu Liu and Yang Liu and Bofang Jia and Han Zhao and Siteng Huang and Zhaoxin Fan and Donglin Wang},
  journal={arXiv preprint arXiv:2403.13358},
  year={2024} -->
<!-- } -->
</code></pre>

            <p class="text-center">Website template borrowed from <a href="http://jonbarron.info/" target="_blank">Jon Barron</a>.</p>
        </div>
    </div>
</div>
</body>
</html>
